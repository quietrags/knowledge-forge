# Knowledge Forge â€” Example Journeys

**Version:** 0.3
**Date:** 2025-12-29

These examples illustrate complete journeys through each mode, showing how the system routes questions, surfaces assumptions, develops concepts, and builds capabilities.

---

## Table of Contents

1. [Build Example: Chrome Extension](#build-example-chrome-extension-that-summarizes-web-pages)
2. [Understand Example: LLM Hallucination](#understand-example-why-do-llms-hallucinate)
3. [Research Example: Local LLMs](#research-example-running-llms-locally)

---

## Build Example: Chrome Extension that Summarizes Web Pages

**User enters:** "I want to build a Chrome extension that summarizes web pages"

### Phase 0: Journey Intake

**Quick Route:** "I want to build" â†’ **Build** âœ“

**Work Backwards:**
- **Ideal answer:** Working extension code + understanding of the architecture + ability to extend/modify it
- **Answer type:** Skill (applied making)
- **Routes to:** Build

**Confirmation:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  You want to BUILD a Chrome extension for page summarization.  â”‚
â”‚                                                                 â”‚
â”‚  I'll help you:                                                â”‚
â”‚  â€¢ Ground yourself in the key concepts (extension architecture,â”‚
â”‚    LLM integration patterns)                                   â”‚
â”‚  â€¢ Identify the constructs you'll combine (manifest, scripts,  â”‚
â”‚    API calls)                                                  â”‚
â”‚  â€¢ Make key decisions (which model, where to process)          â”‚
â”‚  â€¢ Develop the capability to build and extend this             â”‚
â”‚                                                                 â”‚
â”‚              [Let's build â†’]                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 1: Grounding (Minimal Understanding)

**System:** "Before we start building, let's make sure you have the conceptual foundation. I'll keep this minimalâ€”just enough to make good decisions."

#### Grounding Concept 1: Extension Architecture

**System:** "What do you think happens when a Chrome extension runs?"

**User:** "It... adds functionality to the browser? Like a plugin?"

**System:** "Close. Let me sharpen this."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONCEPT: Extension Architecture                                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ An extension is NOT a single program.                          â”‚
â”‚ It's multiple isolated scripts that communicate via messages:  â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     messages     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ Content      â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Background   â”‚           â”‚
â”‚   â”‚ Script       â”‚                  â”‚ Service      â”‚           â”‚
â”‚   â”‚ (per tab)    â”‚                  â”‚ Worker       â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â–²                                  â–²                    â”‚
â”‚         â”‚                                  â”‚                    â”‚
â”‚    page DOM                           API calls                 â”‚
â”‚                                                                 â”‚
â”‚ Content script: Can see the page, can't make API calls         â”‚
â”‚ Service worker: Can make API calls, can't see the page         â”‚
â”‚                                                                 â”‚
â”‚ âš¡ This is why you need message passingâ€”they're isolated.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Grounding Concept 2: Token Economics

**System:** "You'll be sending entire web pages to an LLM. What do you think that costs?"

*Quick research detour:*

| Model | Input (per 1M tokens) | Typical page (~4k tokens) |
|-------|----------------------|---------------------------|
| Claude Haiku | $0.25 | ~$0.001 |
| Claude Sonnet | $3.00 | ~$0.012 |
| GPT-4o-mini | $0.15 | ~$0.0006 |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONCEPT: Token Economics                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ NOT "AI is free after you pay for the API key"                 â”‚
â”‚ IS "Every page summary has a real marginal cost"               â”‚
â”‚                                                                 â”‚
â”‚ Implications for your extension:                                â”‚
â”‚ â€¢ Use cheaper model (Haiku) for most summaries                 â”‚
â”‚ â€¢ Consider caching summaries for revisited pages               â”‚
â”‚ â€¢ Maybe offer "quick" vs "deep" summary options                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Grounding Concept 3: Content Extraction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONCEPT: Content Extraction                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Raw page HTML includes:                                         â”‚
â”‚ â€¢ Navigation, headers, footers (noise)                         â”‚
â”‚ â€¢ Ads, sidebars, related links (noise)                         â”‚
â”‚ â€¢ The actual article content (signal)                          â”‚
â”‚ â€¢ Scripts, styles (garbage)                                    â”‚
â”‚                                                                 â”‚
â”‚ NOT "just grab document.body.innerText"                        â”‚
â”‚ IS "extract the main content, ignore the chrome"               â”‚
â”‚                                                                 â”‚
â”‚ Options:                                                        â”‚
â”‚ â€¢ Mozilla Readability (extracts article content)               â”‚
â”‚ â€¢ Custom selectors (fragile)                                   â”‚
â”‚ â€¢ Send everything, let LLM figure it out (expensive)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Grounding Check

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GROUNDING COMPLETE                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ You now understand:                                             â”‚
â”‚ âœ“ Extension architecture (content script â†” service worker)     â”‚
â”‚ âœ“ Token economics (~$0.001-0.01 per summary)                   â”‚
â”‚ âœ“ Content extraction (signal vs noise)                         â”‚
â”‚                                                                 â”‚
â”‚ This is enough to make informed decisions.                      â”‚
â”‚                                                                 â”‚
â”‚                    [Begin making â†’]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 2: Making (Core Build)

#### Construct 1: Manifest

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONSTRUCT: manifest.json                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ The extension's configuration file. Declares:                  â”‚
â”‚ â€¢ Permissions (activeTab, storage)                             â”‚
â”‚ â€¢ Scripts (content script, service worker)                     â”‚
â”‚ â€¢ UI elements (popup, icon)                                    â”‚
â”‚                                                                 â”‚
â”‚ Usage: Required. Chrome won't load the extension without it.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```json
{
  "manifest_version": 3,
  "name": "Page Summarizer",
  "version": "1.0",
  "permissions": ["activeTab", "storage"],
  "action": {
    "default_popup": "popup.html",
    "default_icon": "icon.png"
  },
  "background": {
    "service_worker": "background.js"
  },
  "content_scripts": [{
    "matches": ["<all_urls>"],
    "js": ["content.js"]
  }]
}
```

#### Construct 2: Content Script

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONSTRUCT: content.js                                           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Runs in the context of each web page.                          â”‚
â”‚ Job: Extract content, send to service worker.                  â”‚
â”‚                                                                 â”‚
â”‚ Usage: Inject Readability, extract article, message background.â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```javascript
// content.js
function extractContent() {
  const documentClone = document.cloneNode(true);
  const reader = new Readability(documentClone);
  const article = reader.parse();

  return {
    title: article?.title || document.title,
    content: article?.textContent || document.body.innerText,
    url: window.location.href
  };
}

chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'extract') {
    sendResponse(extractContent());
  }
});
```

#### Construct 3: Service Worker

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONSTRUCT: background.js (Service Worker)                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Runs in the background, handles API calls.                     â”‚
â”‚ Job: Receive content, call Claude, return summary.             â”‚
â”‚                                                                 â”‚
â”‚ Usage: Message listener + Anthropic API integration.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```javascript
// background.js
async function summarize(content, length = 'medium') {
  const lengthGuide = {
    short: '2-3 sentences',
    medium: '1 paragraph',
    long: '3-4 paragraphs with key points'
  };

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': API_KEY,
      'anthropic-version': '2023-06-01'
    },
    body: JSON.stringify({
      model: 'claude-3-haiku-20240307',
      max_tokens: 500,
      messages: [{
        role: 'user',
        content: `Summarize this article in ${lengthGuide[length]}:\n\n${content}`
      }]
    })
  });

  const data = await response.json();
  return data.content[0].text;
}

chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'summarize') {
    summarize(request.content, request.length)
      .then(summary => sendResponse({ summary }))
      .catch(error => sendResponse({ error: error.message }));
    return true;
  }
});
```

---

#### Decision 1: Which Model?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECISION                                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ âœ“ Choice: Claude Haiku for default summaries                   â”‚
â”‚ âœ— Alternative: Sonnet for all summaries                        â”‚
â”‚                                                                 â”‚
â”‚ Rationale: Haiku is 12x cheaper and summarization doesn't      â”‚
â”‚ require deep reasoning. Offer Sonnet as "deep analysis" option â”‚
â”‚ for complex articles.                                           â”‚
â”‚                                                                 â”‚
â”‚ Combines: [Service Worker, Token Economics understanding]       â”‚
â”‚ Produces: Cost-effective summarization capability               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Decision 2: Where to Store API Key?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECISION                                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ âœ“ Choice: User provides their own API key (stored in           â”‚
â”‚           chrome.storage.local)                                 â”‚
â”‚ âœ— Alternative: Bundle API key in extension / proxy server      â”‚
â”‚                                                                 â”‚
â”‚ Rationale: Bundling key is a security risk (can be extracted). â”‚
â”‚ Proxy server adds complexity and hosting cost. User-provided   â”‚
â”‚ key is safest and makes cost transparent.                      â”‚
â”‚                                                                 â”‚
â”‚ Combines: [Manifest permissions, Storage API]                   â”‚
â”‚ Produces: Secure, user-controlled billing                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Decision 3: When to Summarize?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECISION                                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ âœ“ Choice: On-demand (user clicks button)                       â”‚
â”‚ âœ— Alternative: Auto-summarize every page                       â”‚
â”‚                                                                 â”‚
â”‚ Rationale: Auto-summarize burns tokens on pages user doesn't   â”‚
â”‚ care about. On-demand gives user control over cost.            â”‚
â”‚                                                                 â”‚
â”‚ Produces: Intentional, cost-conscious usage pattern            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Capabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAPABILITY: Summarize any web page on demand                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Enabled by:                                                     â”‚
â”‚ â€¢ Manifest (permissions, script registration)                  â”‚
â”‚ â€¢ Content Script (extracts readable content)                   â”‚
â”‚ â€¢ Service Worker (calls Claude API)                            â”‚
â”‚ â€¢ Decision: Haiku for cost, on-demand for control              â”‚
â”‚                                                                 â”‚
â”‚ You can now:                                                    â”‚
â”‚ â€¢ Click extension icon on any page                             â”‚
â”‚ â€¢ Get a clean summary in seconds                               â”‚
â”‚ â€¢ Choose summary length (short/medium/long)                    â”‚
â”‚ â€¢ Know exactly what each summary costs                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAPABILITY: Extend the extension                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Because you understand the architecture, you can now:          â”‚
â”‚                                                                 â”‚
â”‚ â€¢ Add new features (save summaries, export to Notion)          â”‚
â”‚ â€¢ Change the prompt (bullet points, ELI5, translate)           â”‚
â”‚ â€¢ Add keyboard shortcuts                                        â”‚
â”‚ â€¢ Cache summaries for revisited pages                          â”‚
â”‚ â€¢ Add "deep analysis" mode with Sonnet                         â”‚
â”‚                                                                 â”‚
â”‚ The grounding concepts transfer to any extension you build.    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Build Narrative

> **Building a Chrome Extension for Page Summarization**
>
> This extension combines three constructs: a manifest that declares permissions and scripts, a content script that extracts readable content from any page, and a service worker that calls Claude's API to generate summaries.
>
> Three key decisions shaped the design:
>
> **Model selection:** Haiku over Sonnetâ€”summarization doesn't require deep reasoning, and 12x cost savings matter when summarizing frequently.
>
> **API key management:** User-provided keys stored locally, not bundled or proxied. This is the only secure approach for a client-side extension.
>
> **Trigger pattern:** On-demand, not automatic. Users control their costs by choosing what to summarize.
>
> The grounding phase revealed why these decisions matter: understanding extension architecture (isolated scripts communicating via messages) explains the content-script/service-worker split; understanding token economics justifies the model and trigger choices; understanding content extraction explains why we use Readability instead of raw innerHTML.
>
> With this foundation, extending the extension becomes straightforwardâ€”the same patterns apply to any new feature.

---

### Build Journey Summary

| Phase | What Happened |
|-------|---------------|
| **Intake** | Routed to Build, confirmed intent |
| **Grounding** | 3 concepts: extension architecture, token economics, content extraction |
| **Research detour** | Quick lookup on model pricing |
| **Constructs** | manifest.json, content.js, background.js |
| **Decisions** | Model (Haiku), API key (user-provided), trigger (on-demand) |
| **Capabilities** | Summarize any page, extend the extension |
| **Narrative** | Synthesized the journey into transferable knowledge |

**Key insight:** Everything in grounding served the building. We didn't explore token economics deeplyâ€”just enough to make the Haiku decision. We didn't study extension architecture exhaustivelyâ€”just enough to structure the code.

---

## Understand Example: Why Do LLMs Hallucinate?

**User enters:** "Why do LLMs hallucinate?"

### Phase 0: Journey Intake

**Quick Route:** "Why does X happen?" â†’ **Understand** âœ“

**Work Backwards:**
- **Ideal answer:** A mental model that explains the mechanism behind hallucination, enabling the user to predict when it will happen and understand why it's intrinsic
- **Answer type:** Understanding (causal explanation)
- **Routes to:** Understand (pure conceptualâ€”no making needed)

**Confirmation:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  You want to UNDERSTAND why LLMs hallucinate.                  â”‚
â”‚                                                                 â”‚
â”‚  I'll help you:                                                â”‚
â”‚  â€¢ Surface your current assumptions about how LLMs work        â”‚
â”‚  â€¢ Develop precise concepts that explain the phenomenon        â”‚
â”‚  â€¢ Build a mental model of why hallucination is intrinsic      â”‚
â”‚                                                                 â”‚
â”‚  This is about understanding, not fixing or working around.    â”‚
â”‚                                                                 â”‚
â”‚              [Let's understand â†’]                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 1: Surface Assumptions

**System:** "Before we examine hallucination, let's surface what you currently believe. Complete these sentences honestlyâ€”there are no wrong answers:"

**User responds:**
- "When an LLM hallucinates, it's because *it doesn't have enough information*"
- "The LLM knows it's wrong when *it says 'I'm not sure'*"
- "Hallucination could be fixed by *training on more accurate data*"
- "The LLM's confidence means *it's more likely to be correct*"

**Assumptions surfaced:**

| # | Assumption | Belief Type |
|---|------------|-------------|
| A1 | Hallucination = information gap | Causal |
| A2 | LLM has self-knowledge of correctness | Metacognitive |
| A3 | More/better data = less hallucination | Solutionist |
| A4 | Confidence correlates with correctness | Probabilistic |

---

### Phase 2: Examine Assumptions & Develop Concepts

#### Examining A1: "Hallucination = information gap"

**System:** "If this were true, what would you expect to see?"

**User:** "It would hallucinate about obscure topics but be accurate about common ones?"

**System:** "Let's test that. Ask Claude who won the 1987 Academy Award for Best Picture."

Claude confidently says "The Last Emperor" âœ“ (correct)

**System:** "Now ask who won the 2009 Guildford Chess Congress."

Claude confidently gives a name and result... but it's fabricated.

**System:** "Both questions have 'correct answers' in the training data somewhere. Why did one work and one fail?"

#### Concept 1: Statistical Prediction (Threshold Concept)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš¡ CONCEPT: Statistical Prediction                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Definition:                                                     â”‚
â”‚ An LLM doesn't retrieve factsâ€”it predicts the most likely      â”‚
â”‚ next token given the pattern of previous tokens. "Likely"      â”‚
â”‚ means "frequently followed this pattern in training data."     â”‚
â”‚                                                                 â”‚
â”‚ Distinguished from:                                             â”‚
â”‚ âœ— NOT retrieval (it's not looking things up)                   â”‚
â”‚ âœ— NOT reasoning (it's not deducing from premises)              â”‚
â”‚ âœ— NOT knowing (it has no concept of "true" vs "false")         â”‚
â”‚                                                                 â”‚
â”‚ âš¡ THRESHOLD: Once you see this, you can't unsee it.            â”‚
â”‚ The LLM isn't "wrong"â€”it's doing exactly what it was trained   â”‚
â”‚ to do: produce likely continuations. Sometimes likely â‰  true.  â”‚
â”‚                                                                 â”‚
â”‚ From assumption: A1 ("doesn't have enough information")        â”‚
â”‚ Status: DISCARDED                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Examining A2: "LLM knows when it's wrong"

**System:** "When Claude says 'I'm not sure,' what do you think is happening?"

**User:** "It's... checking its confidence? Recognizing uncertainty?"

**System:** "What makes YOU say 'I'm not sure'?"

**User:** "I notice a feeling of uncertainty. I'm aware I might be wrong."

**System:** "That requires two things: (1) a sense that there's a correct answer, and (2) awareness that your internal state doesn't match it. Does an LLM have either?"

#### Concept 2: Performed Uncertainty

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONCEPT: Performed Uncertainty                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Definition:                                                     â”‚
â”‚ When an LLM says "I'm not sure" or "I might be wrong," it's   â”‚
â”‚ generating text that *resembles* uncertaintyâ€”not experiencing  â”‚
â”‚ or detecting actual uncertainty.                               â”‚
â”‚                                                                 â”‚
â”‚ Distinguished from:                                             â”‚
â”‚ âœ— NOT metacognition (no awareness of its own states)           â”‚
â”‚ âœ— NOT calibrated confidence (no correlation with correctness)  â”‚
â”‚ âœ— NOT humility (no sense of limitation)                        â”‚
â”‚                                                                 â”‚
â”‚ Implication:                                                    â”‚
â”‚ You cannot trust "I'm not sure" as a signal of actual         â”‚
â”‚ uncertainty. The model might be confidently wrong or          â”‚
â”‚ uncertainly correctâ€”the words don't tell you which.           â”‚
â”‚                                                                 â”‚
â”‚ From assumption: A2 ("LLM knows when it's wrong")              â”‚
â”‚ Status: DISCARDED                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Examining A3: "Better data = fix"

**System:** "If we trained only on verified facts, would hallucination disappear?"

*Research detour:*

**Findings:**
- Models trained on Wikipedia + textbooks still hallucinate
- Hallucination rate decreases but never reaches zero
- Novel combinations of true facts can produce false statements
- The generation mechanism itself produces hallucinations

#### Concept 3: Generative Confabulation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONCEPT: Generative Confabulation                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Definition:                                                     â”‚
â”‚ The act of producing coherent, plausible text that combines    â”‚
â”‚ elements in ways not grounded in any training example.         â”‚
â”‚ The model doesn't "make mistakes"â€”it generates novel           â”‚
â”‚ combinations that happen to not correspond to reality.         â”‚
â”‚                                                                 â”‚
â”‚ Distinguished from:                                             â”‚
â”‚ âœ— NOT errors (implies a correct process gone wrong)            â”‚
â”‚ âœ— NOT lies (implies intent to deceive)                         â”‚
â”‚ âœ— NOT guesses (implies awareness of uncertainty)               â”‚
â”‚                                                                 â”‚
â”‚ Why it can't be "fixed":                                       â”‚
â”‚ The same mechanism that produces creative, useful text         â”‚
â”‚ (novel combinations) also produces hallucinations.             â”‚
â”‚ You can't have one without the other.                          â”‚
â”‚                                                                 â”‚
â”‚ From assumption: A3 ("better data = fix")                      â”‚
â”‚ Status: DISCARDED                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Examining A4: "Confidence = correctness"

**System:** "When Claude states something without hedging, do you assume it's more likely to be true?"

**User:** "Yes, intuitively."

**System:** "The confidence is in the *text pattern*, not the *factual accuracy*."

#### Concept 4: Surface Confidence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONCEPT: Surface Confidence                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ Definition:                                                     â”‚
â”‚ The stylistic quality of text that sounds certain. Determined  â”‚
â”‚ by training patterns (academic writing sounds confident,       â”‚
â”‚ hedging appears in certain contexts) not by factual accuracy.  â”‚
â”‚                                                                 â”‚
â”‚ Distinguished from:                                             â”‚
â”‚ âœ— NOT calibrated probability (no correlation with truth)       â”‚
â”‚ âœ— NOT model confidence scores (those exist but aren't shown)   â”‚
â”‚ âœ— NOT expertise (a model can sound confident about anything)   â”‚
â”‚                                                                 â”‚
â”‚ Why it's misleading:                                           â”‚
â”‚ Confident text patterns occur in training for TRUE statements. â”‚
â”‚ The model learns to produce confident style for all outputs,   â”‚
â”‚ regardless of whether the specific claim is accurate.          â”‚
â”‚                                                                 â”‚
â”‚ From assumption: A4 ("confidence = correctness")               â”‚
â”‚ Status: DISCARDED                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 3: Integrate into Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODEL: Why LLMs Hallucinate                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                    â”‚ Training Corpus â”‚                         â”‚
â”‚                    â”‚ (patterns, not  â”‚                         â”‚
â”‚                    â”‚  facts)         â”‚                         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                    â”‚ Statistical     â”‚                         â”‚
â”‚                    â”‚ Prediction      â”‚                         â”‚
â”‚                    â”‚ "What token is  â”‚                         â”‚
â”‚                    â”‚  likely next?"  â”‚                         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                             â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â–¼                              â–¼                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ Likely + True   â”‚            â”‚ Likely + False  â”‚          â”‚
â”‚    â”‚ (useful output) â”‚            â”‚ (hallucination) â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚    SAME MECHANISM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º SAME MECHANISM  â”‚
â”‚                                                                 â”‚
â”‚    No internal signal distinguishes these two cases.           â”‚
â”‚    Surface confidence applied equally to both.                 â”‚
â”‚    Performed uncertainty uncorrelated with which is which.     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CORE INSIGHT:
Hallucination is not a failure mode. It's the inevitable result of
statistical prediction applied to factual questions. The model has
no concept of "true"â€”only "likely." When likely â‰  true, you get
hallucination. When likely = true, you get useful output. Same process.
```

---

### Analysis Essay

> **Why LLMs Hallucinate**
>
> Hallucination isn't a bugâ€”it's an intrinsic property of how large language models work. Understanding why requires discarding several intuitive but incorrect assumptions.
>
> **The core mechanism: Statistical Prediction**
>
> An LLM doesn't store facts and retrieve them. It predicts the most likely next token given the previous tokens. "Likely" means "frequently followed this pattern in training." This is fundamentally different from knowing, reasoning, or remembering.
>
> When you ask about the 1987 Best Picture winner, the pattern "1987 Best Picture" was strongly associated with "The Last Emperor" in training dataâ€”many articles, reviews, and lists reinforced this. The prediction aligns with truth.
>
> When you ask about an obscure chess tournament, there's no strong pattern. The model still produces confident textâ€”because that's what it doesâ€”but the prediction doesn't align with any ground truth. Same mechanism, different outcome.
>
> **Why confidence is misleading**
>
> The style of confident assertion is itself a pattern learned from training. Academic papers, encyclopedia entries, and authoritative sources don't hedge on facts. The model learned to produce this style, regardless of whether the specific claim is accurate. Surface confidence is a text pattern, not a reliability signal.
>
> **Why uncertainty expressions don't help**
>
> When Claude says "I'm not sure," it's not experiencing uncertaintyâ€”it's producing text that resembles uncertainty. This happens when the context suggests hedging (e.g., after being challenged) not when the underlying "knowledge" is actually uncertain. You cannot use these phrases to gauge reliability.
>
> **Why it can't be "fixed"**
>
> The same generative capability that produces useful, creative, novel outputsâ€”code that works, explanations that clarify, writing that engagesâ€”is the capability that produces hallucinations. Both are novel combinations of learned patterns. You cannot have a model that creatively combines ideas without sometimes combining them into false claims.
>
> **The transformed understanding**
>
> Stop asking "how do we make LLMs stop hallucinating?" Start asking "how do we work with systems that produce plausible text without regard for truth?" The first question has no answer. The second has many: verification, grounding, structured outputs, retrieval augmentationâ€”but none eliminate hallucination, because hallucination isn't a failure. It's the system working exactly as designed.

---

### Assumptions Summary

| Assumption | Status | Replaced With |
|------------|--------|---------------|
| A1: Hallucination = information gap | âŒ Discarded | Statistical predictionâ€”no "information" involved |
| A2: LLM knows when it's wrong | âŒ Discarded | Performed uncertaintyâ€”no metacognition |
| A3: Better data = fix | âŒ Discarded | Generative confabulationâ€”intrinsic to the mechanism |
| A4: Confidence = correctness | âŒ Discarded | Surface confidenceâ€”text style, not reliability |

---

### Understand Journey Summary

| Phase | What Happened |
|-------|---------------|
| **Intake** | Routed to Understand (causal question, no making needed) |
| **Surface** | 4 assumptions identified through sentence completion |
| **Examine A1** | Tested with Oscar vs chess tournament examples |
| **Concept 1** | Statistical Prediction âš¡ (threshold concept) |
| **Examine A2** | Probed what "I'm not sure" actually means |
| **Concept 2** | Performed Uncertainty |
| **Examine A3** | Research detour on curated training data |
| **Concept 3** | Generative Confabulation |
| **Examine A4** | Tested confidence vs correctness correlation |
| **Concept 4** | Surface Confidence |
| **Integrate** | Mental model: same mechanism â†’ useful or hallucination |
| **Essay** | Synthesized transformed understanding |

**Key insight:** There's no artifact. No code. No "how to deal with hallucination." The user now *understands* hallucination in a way that transforms how they think about LLMsâ€”but if they wanted to *do* something about it, that would be a separate Build journey.

---

## Research Example: Running LLMs Locally

**User enters:** "What are the current options for running LLMs locally?"

### Phase 0: Journey Intake

**Quick Route:** "What are the options for X?" â†’ **Research** âœ“

**Work Backwards:**
- **Ideal answer:** A comprehensive landscape of local LLM optionsâ€”models, runtimes, hardware requirementsâ€”organized and compared
- **Answer type:** Facts (survey/comparison)
- **Routes to:** Research (primaryâ€”user wants knowledge gathered)

**Confirmation:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  This is a RESEARCH questionâ€”you want to survey the landscape â”‚
â”‚  of local LLM options.                                         â”‚
â”‚                                                                 â”‚
â”‚  I'll help you:                                                â”‚
â”‚  â€¢ Decompose this into specific, answerable questions          â”‚
â”‚  â€¢ Find and evaluate sources for each                          â”‚
â”‚  â€¢ Synthesize findings into key insights                       â”‚
â”‚  â€¢ Identify adjacent questions for further exploration         â”‚
â”‚                                                                 â”‚
â”‚  [Begin Research â†’]    [I want to build something with this â†’] â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 1: Decompose into Questions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TOPIC: Running LLMs Locally                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ ğŸ“‚ Category: Available Models                                   â”‚
â”‚    â”œâ”€ â—‹ What open-source LLMs can run locally?                 â”‚
â”‚    â”œâ”€ â—‹ How do model sizes affect requirements?                â”‚
â”‚    â””â”€ â—‹ What are quantized models?                             â”‚
â”‚                                                                 â”‚
â”‚ ğŸ“‚ Category: Inference Runtimes                                 â”‚
â”‚    â”œâ”€ â—‹ What software runs local LLMs?                         â”‚
â”‚    â””â”€ â—‹ How do llama.cpp, Ollama, and vLLM compare?           â”‚
â”‚                                                                 â”‚
â”‚ ğŸ“‚ Category: Hardware Requirements                              â”‚
â”‚    â”œâ”€ â—‹ What GPU/RAM is needed for different model sizes?      â”‚
â”‚    â””â”€ â—‹ Can you run LLMs on CPU only?                          â”‚
â”‚                                                                 â”‚
â”‚ ğŸ“‚ Category: Comparison to Cloud                                â”‚
â”‚    â”œâ”€ â—‹ How does local quality compare to Claude/GPT-4?        â”‚
â”‚    â””â”€ â—‹ What are the cost tradeoffs?                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 2: Answer Questions

#### Category: Available Models

**Q: What open-source LLMs can run locally?**

| Model Family | Creator | Sizes | License |
|--------------|---------|-------|---------|
| Llama 3.1 | Meta | 8B, 70B, 405B | Llama 3.1 License |
| Mistral | Mistral AI | 7B, 8x7B (Mixtral) | Apache 2.0 |
| Qwen 2.5 | Alibaba | 0.5B - 72B | Apache 2.0 |
| Gemma 2 | Google | 2B, 9B, 27B | Gemma License |
| Phi-3 | Microsoft | 3.8B, 14B | MIT |

**Sources:** [HF Leaderboard: high] [LMSys Arena: high]

**Q: How do model sizes affect requirements?**

| Model Size | VRAM (FP16) | VRAM (Q4) | RAM (CPU) |
|------------|-------------|-----------|-----------|
| 7-8B | 16 GB | 4-6 GB | 8-16 GB |
| 13-14B | 28 GB | 8-10 GB | 16-32 GB |
| 30-35B | 70 GB | 20-24 GB | 64 GB |
| 70B | 140 GB | 40-48 GB | 128 GB |

**Sources:** [llama.cpp docs: primary] [Ollama: primary]

**Q: What are quantized models?**

Models compressed from 16-bit to 4/8-bit precision. Reduces VRAM 2-4x with ~1-3% quality loss. GGUF (llama.cpp) and AWQ/GPTQ are main formats.

**Rise Above:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“‚ Category: Available Models                                   â”‚
â”‚ INSIGHT: The "Llama 3.1 8B quantized" sweet spot               â”‚
â”‚                                                                 â”‚
â”‚ For most local use cases, Llama 3.1 8B with Q4 quantization   â”‚
â”‚ hits the sweet spot: fits in 6GB VRAM (consumer GPU), performs â”‚
â”‚ close to GPT-3.5 on most tasks.                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Category: Inference Runtimes

**Q: What software runs local LLMs?**

| Runtime | Type | Best For |
|---------|------|----------|
| llama.cpp | C++ inference | Maximum compatibility, CPU |
| Ollama | Wrapper + API | Easy setup, docker-like |
| vLLM | Python server | Production serving, batching |
| LM Studio | Desktop app | GUI, beginners |

**Q: How do llama.cpp, Ollama, and vLLM compare?**

| Aspect | llama.cpp | Ollama | vLLM |
|--------|-----------|--------|------|
| Setup | Compile | `brew install` | pip + GPU |
| Use Case | Tinkering, CPU | Personal use | Production |

**Rise Above:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“‚ Category: Inference Runtimes                                 â”‚
â”‚ INSIGHT: The abstraction ladder                                 â”‚
â”‚                                                                 â”‚
â”‚ llama.cpp â†’ Ollama â†’ Your app                                  â”‚
â”‚ (engine)    (ease)   (integration)                             â”‚
â”‚                                                                 â”‚
â”‚ Start with Ollama. Drop to llama.cpp if you need control.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Category: Hardware Requirements

**Q: What GPU/RAM is needed for different model sizes?**

| Setup | Can Run | Quality Level |
|-------|---------|---------------|
| M1/M2 Mac 16GB | 7B Q8, 13B Q4 | Good |
| RTX 3060 12GB | 7B Q8, 13B Q4 | Good, fast |
| RTX 4090 24GB | 30B Q4 | Excellent, fast |

**Q: Can you run LLMs on CPU only?**

Yes. llama.cpp does ~10-20 tok/s on CPU for 7B Q4. Usable for testing, too slow for real-time chat.

**Rise Above:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“‚ Category: Hardware Requirements                              â”‚
â”‚ INSIGHT: The Mac surprise                                       â”‚
â”‚                                                                 â”‚
â”‚ Apple Silicon unified memory makes Macs competitive local LLM  â”‚
â”‚ platforms. An M2 Pro 32GB can run models that need expensive   â”‚
â”‚ discrete GPUs on Windows/Linux.                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Category: Comparison to Cloud

**Q: How does local quality compare to Claude/GPT-4?**

| Model | Relative Quality |
|-------|------------------|
| GPT-4 / Claude 3.5 Sonnet | 100% (reference) |
| Llama 3.1 70B | ~80-85% |
| Llama 3.1 8B | ~60-70% |

**Q: What are the cost tradeoffs?**

Local is "free" per query after hardware cost. Break-even vs API at ~500K-2M tokens. But local models are weakerâ€”you're trading quality for cost, plus you get privacy and no rate limits.

**Rise Above:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“‚ Category: Comparison to Cloud                                â”‚
â”‚ INSIGHT: Different tools for different jobs                    â”‚
â”‚                                                                 â”‚
â”‚ Local isn't "cheaper Claude"â€”it's a different tool:           â”‚
â”‚ â€¢ Privacy (data never leaves your machine)                     â”‚
â”‚ â€¢ No rate limits                                               â”‚
â”‚ â€¢ Offline capability                                           â”‚
â”‚ â€¢ Customization (fine-tuning)                                  â”‚
â”‚                                                                 â”‚
â”‚ Use local for: bulk processing, privacy, experimentation.      â”‚
â”‚ Use cloud for: maximum quality, complex reasoning.             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 3: Key Insights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KEY INSIGHTS                                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ ğŸ’¡ The Llama 3.1 8B Q4 sweet spot                              â”‚
â”‚    Best balance of capability/requirements for most users.     â”‚
â”‚                                                                 â”‚
â”‚ ğŸ’¡ The abstraction ladder                                       â”‚
â”‚    llama.cpp â†’ Ollama â†’ Your app. Start with Ollama.          â”‚
â”‚                                                                 â”‚
â”‚ ğŸ’¡ The Mac surprise                                             â”‚
â”‚    Apple Silicon is surprisingly competitive.                  â”‚
â”‚                                                                 â”‚
â”‚ ğŸ’¡ Different tools for different jobs                          â”‚
â”‚    Local = privacy, no limits. Cloud = quality, reliability.  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 4: Frontier (Adjacent Questions)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTIER: Adjacent Questions                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                 â”‚
â”‚ ğŸ”­ Discovered from "What are quantized models?"                â”‚
â”‚    ? How does quantization actually work (AWQ vs GPTQ)?        â”‚
â”‚    ? Can you quantize a model yourself?                        â”‚
â”‚                                                                 â”‚
â”‚ ğŸ”­ Discovered from "How do runtimes compare?"                  â”‚
â”‚    ? How do you deploy a local LLM as an API?                  â”‚
â”‚    ? What's the Ollama Modelfile format?                       â”‚
â”‚                                                                 â”‚
â”‚ ğŸ”­ Discovered from "Cost tradeoffs"                            â”‚
â”‚    ? Is fine-tuning local models practical?                    â”‚
â”‚    ? What about running on cloud GPUs you control?             â”‚
â”‚                                                                 â”‚
â”‚ ğŸ”­ Discovered from "Quality comparison"                        â”‚
â”‚    ? Why do local models lag on reasoning tasks?               â”‚
â”‚    ? What's the state of local code models specifically?       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Research Essay

> **Running LLMs Locally: A Landscape Survey**
>
> The local LLM ecosystem has matured rapidly. Where running your own model once required deep ML expertise, today anyone can run capable language models on consumer hardware with a single command.
>
> **The model landscape**
>
> Llama 3.1 from Meta dominates open-source. The 8B parameter version, when quantized to 4-bit precision, runs on a 6GB GPU while performing near GPT-3.5 levels. For those with more hardware, the 70B version approaches GPT-4 on many tasks.
>
> **The software stack**
>
> Most local LLM software builds on llama.cpp. Ollama wraps this in a Docker-like experienceâ€”`ollama run llama3.1` is all you need. For production serving, vLLM is the standard.
>
> **Hardware requirements**
>
> A 7B quantized model runs on a $300 GPU or 16GB Mac. Apple Silicon, with unified memory, has become surprisingly competitiveâ€”an M2 Pro can run models that would need expensive discrete GPUs elsewhere.
>
> **When to use local vs cloud**
>
> Local LLMs aren't "cheaper Claude"â€”they're different tools. Choose local for: privacy-sensitive data, bulk processing, offline use, experimentation. Choose cloud for: maximum quality, complex reasoning, simplicity.
>
> **The practical starting point**
>
> Install Ollama, run Llama 3.1 8B, see how far it gets you. The ecosystem has made the on-ramp gentle enough that experimentation costs almost nothing.

---

### Research Journey Summary

| Phase | What Happened |
|-------|---------------|
| **Intake** | Routed to Research (landscape survey question) |
| **Decompose** | Topic â†’ 4 categories, 9 questions |
| **Answer** | Each question researched with sources |
| **Rise Above** | 4 category insights synthesized |
| **Key Insights** | Cross-cutting insights extracted |
| **Frontier** | 8 adjacent questions discovered |
| **Essay** | Synthesized findings into coherent narrative |

---

## Comparison: Three Modes

| Aspect | Research | Understand | Build |
|--------|----------|------------|-------|
| **Goal** | Gather knowledge | Transform thinking | Create capability |
| **Output** | Answered questions + synthesis | Concepts + mental model | Constructs + decisions |
| **Assumptions** | Not surfaced | Surfaced and examined | Minimal (grounding only) |
| **Artifacts** | None | None | Code/system |
| **"Rise above"** | Category insights | Model integration | Capability composition |
| **Frontier** | Adjacent questions | Transferred understanding | Extended system |

---

## What Comes Next

Each mode can lead to another:

**Research â†’ Build:**
> "Now help me set up Ollama for my team"

**Research â†’ Understand:**
> "Why do local models lag on reasoning?"

**Understand â†’ Build:**
> "Now help me build guardrails for hallucination"

**Build â†’ Research:**
> (During grounding) "What are the API rate limits?"

**Build â†’ Understand:**
> (When stuck) "Why isn't the message passing working?"

The modes aren't isolatedâ€”they're a connected system for learning.
